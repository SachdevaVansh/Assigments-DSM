{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2fff6fc-2ccf-4b16-89a4-ecf943f3b6ae",
   "metadata": {},
   "source": [
    "#### Question 1-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019fe3ea-a22c-4c36-a903-0d9175827390",
   "metadata": {},
   "source": [
    "#### Underfitting-\n",
    "* A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the underlying trend of the data, i.e., it only performs well on training data but performs poorly on testing data.\n",
    "* Underfitting destroys the accuracy of our machine learning model. Its occurrence simply means that our model or the algorithm does not fit the data well enough. It usually happens when we have fewer data to build an accurate model and also when we try to build a linear model with fewer non-linear data. In such cases, the rules of the machine learning model are too easy and flexible to be applied to such minimal data and therefore the model will probably make a lot of wrong predictions. \n",
    "* Underfitting can be avoided by using more data and also reducing the features by feature selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b94965-c433-4ffe-b2ee-3e8d402d3dae",
   "metadata": {},
   "source": [
    "#### Overfitting-\n",
    "\n",
    "* A statistical model is said to be overfitted when the model does not make accurate predictions on testing data. When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing with test data results in High variance. Then the model does not categorize the data correctly, because of too many details and noise. \n",
    "* The causes of overfitting are the non-parametric and non-linear methods because these types of machine learning algorithms have more freedom in building the model based on the dataset and therefore they can really build unrealistic models.\n",
    "* A solution to avoid overfitting is using a linear algorithm if we have linear data or using the parameters like the maximal depth if we are using decision trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bf9087-b7b7-402b-b0c1-0832961c9026",
   "metadata": {},
   "source": [
    "#### Question 2-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c417a98-9ab2-4f13-a24a-5f984c09e7a9",
   "metadata": {},
   "source": [
    "#### Reasons for Overfitting are as follows:\n",
    "* High variance and low bias \n",
    "* The model is too complex\n",
    "* The size of the training data \n",
    "\n",
    "#### Techniques to reduce overfitting:\n",
    "\n",
    "* Increase training data.\n",
    "* Reduce model complexity.\n",
    "* Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\n",
    "* Ridge Regularization and Lasso Regularization\n",
    "* Use dropout for neural networks to tackle overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795b24f-8381-4e0f-8c13-d1e61eb8ff9e",
   "metadata": {},
   "source": [
    "#### Question 3-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda5a1ea-dc98-4714-8774-262e5e27a41c",
   "metadata": {},
   "source": [
    "Underfitting-\n",
    "\n",
    "When a model has not learned the patterns in the training data well and is unable to generalize well on the new data, it is known as underfitting. An underfit model has poor performance on the training data and will result in unreliable predictions. Underfitting occurs due to high bias and low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a6a9e-f0e4-4d1c-bbd3-b15f011bda17",
   "metadata": {},
   "source": [
    "#### Reasons for Underfitting\n",
    "* Data used for training is not cleaned and contains noise (garbage values) in it\n",
    "* The model has a high bias\n",
    "* The size of the training dataset used is not enough\n",
    "* The model is too simple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c402f8d0-9281-425f-8d3a-e20dfde259c7",
   "metadata": {},
   "source": [
    "#### Question 4-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337ecc5c-02af-47cc-a607-87fcd12cef80",
   "metadata": {},
   "source": [
    "It is important to understand prediction errors (bias and variance) when it comes to accuracy in any machine learning algorithm. There is a tradeoff between a model’s ability to minimize bias and variance which is referred to as the best solution for selecting a value of Regularization constant. Proper understanding of these errors would help to avoid the overfitting and underfitting of a data set while training the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b9fba0-a43e-4dbd-8456-b9753dd1d2c5",
   "metadata": {},
   "source": [
    "Bias-Variance Tradeoff-\n",
    "\n",
    "If our model is too simple and has very few parameters then it may have high bias and low variance. On the other hand if our model has large number of parameters then it’s going to have high variance and low bias. So we need to find the right/good balance without overfitting and underfitting the data.\n",
    "\n",
    "This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad6209-fdcf-4388-b6bf-541124dedfa1",
   "metadata": {},
   "source": [
    "#### Question 5-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f46afcb-85a8-41f6-9939-a2beac6113c9",
   "metadata": {},
   "source": [
    "Detecting overfitting/underfitting is almost impossible before you test the data. It can help address the inherent characteristic of overfitting/underfitting, which is the inability to generalize data sets. The data can, therefore, be separated into different subsets to make it easy for training and testing. The data is split into two main parts, i.e., a test set and a training set.\n",
    "\n",
    "The training set represents a majority of the available data (about 80%), and it trains the model. The test set represents a small portion of the data set (about 20%), and it is used to test the accuracy of the data it never interacted with before. By segmenting the dataset, we can examine the performance of the model on each set of data to spot overfitting/underfitting when it occurs, as well as see how the training process works.\n",
    "\n",
    "The performance can be measured using the percentage of accuracy observed in both data sets to conclude on the presence of overfitting/underfitting. If the model performs better on the training set than on the test set, it means that the model is likely overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2258db63-b1c0-4bdd-a542-b62dd60e69de",
   "metadata": {},
   "source": [
    "#### Question 6-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d77b56-222d-46bc-af2e-a82c38d8602a",
   "metadata": {},
   "source": [
    "#### Bias--->\n",
    "a.)definition-\n",
    "* Every machine learning algorithm has a prediction error, which can be segmented into three subcomponents: bias error, variance error, and irreducible error. In the process of machine learning, faulty assumptions can lead to the occurrence of a phenomena known as bias.\n",
    "\n",
    "b.)values-\n",
    "* The disparity between the values that were predicted and the values that were actually observed is referred to as bias.\n",
    "\n",
    "c.)data-\n",
    "* The model is incapable of locating patterns in the dataset that it was trained on, and it produces inaccurate results for both seen and unseen data.\n",
    "\n",
    "#### Variance--->\n",
    "a.)definition-\n",
    "* The difference in the accuracy of a machine learning model's predictions between the training data and the test data is referred to as variance. A variance error is what we call the situation when a change in the performance of the model is brought about by a variation in the dataset.\n",
    "\n",
    "b.)values-\n",
    "* A random variable's variance is a measure of how much it varies from the value that was predicted for it.\n",
    "\n",
    "c.)data-\n",
    "* The model recognizes the majority of the dataset's patterns and can even learn from the noise or data that isn't vital to its operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1e909e-949e-4e91-9366-e8ea45aa8fd8",
   "metadata": {},
   "source": [
    "It's not typical to have high degrees of both, but if it happens it's neither overfitting nor underfitting because these are defined with contrasting amounts of bias and variance. An overfitting model performs very well on the dataset it learns.\n",
    "\n",
    "* High bias, high variance: Aiming off the target and being inconsistent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1e1842-ebc9-4e91-95e7-1179d65cbbb4",
   "metadata": {},
   "source": [
    "#### Question 7-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf90cd-efba-433d-8893-b7d7dbeff8a0",
   "metadata": {},
   "source": [
    "Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the adjusted loss function and prevent overfitting or underfitting.\n",
    "\n",
    "Using Regularization, we can fit our machine learning model appropriately on a given test set and hence reduce the errors in it. \n",
    "\n",
    "There are two main types of regularization techniques: Ridge Regularization and Lasso Regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db436df7-1429-4856-ac14-4804baa7191f",
   "metadata": {},
   "source": [
    "#### Ridge Regularization : \n",
    "\n",
    "* Also known as Ridge Regression, it modifies the over-fitted or under fitted models by adding the penalty equivalent to the sum of the squares of the magnitude of coefficients.\n",
    "* This means that the mathematical function representing our machine learning model is minimized and coefficients are calculated. The magnitude of coefficients is squared and added. Ridge Regression performs regularization by shrinking the coefficients present.\n",
    "\n",
    "#### Lasso Regression :\n",
    "\n",
    "* It modifies the over-fitted or under-fitted models by adding the penalty equivalent to the sum of the absolute values of coefficients. \n",
    "\n",
    "* Lasso regression also performs coefficient minimization,  but instead of squaring the magnitudes of the coefficients, it takes the true values of coefficients. This means that the coefficient sum can also be 0, because of the presence of negative coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e99d44-3903-4d48-ba83-429efaba7568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
