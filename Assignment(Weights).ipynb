{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Understanding Weight initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the importance of weight initialization in artificial neural networks. Why is it necessary to initialze the weights carefully?\n",
    "\n",
    "Ans: Weight initialization is crucial in artificial neural networks as it sets the starting values for the model's weights. \n",
    "- Careful initialization ensures that the network begins training with reasonable weights, preventing issues like vanishing/exploding gradients. \n",
    "- Proper initialization improves convergence and overall performance during training.\n",
    "\n",
    "2. Describe the challenges associated with improper weight initialization. How do these issues affect model training and convergence?\n",
    "\n",
    "Ans:  Improper weight initialization can lead to challenges like vanishing or exploding gradients. Vanishing gradients make the network hard to train as updates become negligible, while exploding gradients cause instability and divergence. Both issues hinder model convergence, resulting in slow learning or failure to learn altogether.\n",
    "\n",
    "3. Discuss the concept of variance and how it relates to weight initialization. Why is it crucial to consider the variance of weights during initialization?\n",
    "\n",
    "Ans: Variance in weight initialization refers to the spread or range of weight values. Proper initialization adjusts the variance to ensure that the weights are not too large or too small, allowing gradients to flow well during training.\n",
    "Balancing the variance is crucial for stable and effective learning in neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Weight Initialization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Explain the concept of zero initialization. Discuss its potential limitations and when it can be appropriate to use.\n",
    "    \n",
    "Ans: Zero initialization sets all the weights in a neural network to zero initially. However, this approach has limitations as all neurons learn the same features, and symmetry-breaking is absent, leading to poor learning. Zero initialization can be suitable for biases in certain cases, but it's generally not recommended for weights in hidden layers.\n",
    "\n",
    "5. Describe the process of random initialization. How can random initialization be adjusted to mitigate potential issues like saturation or vanishing/exploding gradients? \n",
    "   \n",
    "Ans: Random initialization sets weights to random values within a defined range. To mitigate saturation or vanishing/exploding gradients, use appropriate variance adjustment. For example, in Xavier initialization, scale weights based on the number of input and output units to maintain a balance between activations and gradients during training.\n",
    "\n",
    "6. Discuss the concept of Xavier/Glorot initialization. Explain how it addresses the challenges of improper weight initialization and the underlying theory behind it. \n",
    "    \n",
    "Ans: Xavier/Glorot initialization is a weight initialization technique that sets weights using a carefully chosen variance. It balances the initialization to avoid vanishing/exploding gradients during training. The theory is based on ensuring the variance of activations and gradients remains consistent across layers,leading to more stable and efficient learning in neural networks.\n",
    "\n",
    "7. Explain the concept of He initialization. How does it differ from Xavier initialization, and when is it preferred?\n",
    "    \n",
    "Ans: He initialization is a weight initialization method optimized for ReLU activation functions. It sets weights with higher variance, helping ReLU neurons to overcome vanishing gradients and learn better. Unlike Xavier, it uses variance based solely on the number of input units, making it suitable for deeper networks with ReLU activations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Applying Weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-26 18:56:54.010459: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 18:56:54.544596: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 18:56:54.547002: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-26 18:56:56.323553: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "\n",
    "def model(INITIALIZER, AF):\n",
    "    # loading dataset\n",
    "    (X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range\n",
    "    X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "    y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "    # scale the test set as well\n",
    "    X_test = X_test / 255.\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(Flatten(input_shape=[28, 28], name=\"inputLayer\"))\n",
    "    model.add(Dense(300, activation=AF, name=\"hiddenLayer1\",kernel_initializer=INITIALIZER))\n",
    "    model.add(Dense(100, activation=AF, name=\"hiddenLayer2\",kernel_initializer=INITIALIZER))\n",
    "    model.add(Dense(10, activation=\"softmax\", name=\"outputLayer\"))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=[\"accuracy\"])\n",
    "    model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid), batch_size=1000)\n",
    "    \n",
    "    return max(model.history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55/55 [==============================] - 2s 14ms/step - loss: 2.3022 - accuracy: 0.1117 - val_loss: 2.3017 - val_accuracy: 0.1126\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 2.3016 - accuracy: 0.1123 - val_loss: 2.3013 - val_accuracy: 0.1126\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 2.3014 - accuracy: 0.1123 - val_loss: 2.3011 - val_accuracy: 0.1126\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 2.3013 - accuracy: 0.1123 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 2.3012 - accuracy: 0.1123 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 2.3012 - accuracy: 0.1123 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 2.3012 - accuracy: 0.1123 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 2.3012 - accuracy: 0.1123 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 2.3012 - accuracy: 0.1123 - val_loss: 2.3009 - val_accuracy: 0.1126\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 2.3012 - accuracy: 0.1123 - val_loss: 2.3009 - val_accuracy: 0.1126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55/55 [==============================] - 2s 14ms/step - loss: 6831.2642 - accuracy: 0.1037 - val_loss: 573.8668 - val_accuracy: 0.0976\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 486.9743 - accuracy: 0.1144 - val_loss: 305.5351 - val_accuracy: 0.1070\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 274.4095 - accuracy: 0.1426 - val_loss: 192.7481 - val_accuracy: 0.3014\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 192.5676 - accuracy: 0.1900 - val_loss: 180.1408 - val_accuracy: 0.2270\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 145.7617 - accuracy: 0.2540 - val_loss: 39.1574 - val_accuracy: 0.3736\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 11.0099 - accuracy: 0.6811 - val_loss: 5.5738 - val_accuracy: 0.7682\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 5.6908 - accuracy: 0.7659 - val_loss: 6.0619 - val_accuracy: 0.7526\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 5.2847 - accuracy: 0.7767 - val_loss: 4.3167 - val_accuracy: 0.7942\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 4.2421 - accuracy: 0.8018 - val_loss: 4.1892 - val_accuracy: 0.8034\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 4.3391 - accuracy: 0.7988 - val_loss: 3.4160 - val_accuracy: 0.8178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.6606 - accuracy: 0.8239 - val_loss: 0.2494 - val_accuracy: 0.9264\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.2279 - accuracy: 0.9341 - val_loss: 0.1762 - val_accuracy: 0.9508\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.1653 - accuracy: 0.9525 - val_loss: 0.1428 - val_accuracy: 0.9612\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.1295 - accuracy: 0.9629 - val_loss: 0.1204 - val_accuracy: 0.9654\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.1048 - accuracy: 0.9699 - val_loss: 0.1044 - val_accuracy: 0.9698\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.0854 - accuracy: 0.9752 - val_loss: 0.0927 - val_accuracy: 0.9734\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.0709 - accuracy: 0.9798 - val_loss: 0.0865 - val_accuracy: 0.9738\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.0618 - accuracy: 0.9822 - val_loss: 0.0813 - val_accuracy: 0.9756\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.0508 - accuracy: 0.9857 - val_loss: 0.0740 - val_accuracy: 0.9770\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.0431 - accuracy: 0.9881 - val_loss: 0.0733 - val_accuracy: 0.9774\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zero_initializers : 0.11259999871253967\n",
      "random_Uniform_initializers : 0.817799985408783\n",
      "random_Normal_initializers : 0.977400004863739\n"
     ]
    }
   ],
   "source": [
    "# 8. Implement different weight initialization techniques (zero initialization, random initialization, Xavier initialization, and He initialization) in a neural network using a framework of your choice. Train the model on a suitable dataset and compare the performance of the initialized models. \n",
    "\n",
    "initializers = {\n",
    "    'zero_initializers' : model(tf.keras.initializers.Zeros(), 'relu'),\n",
    "    'random_Uniform_initializers' : model(tf.keras.initializers.RandomUniform(minval=0., maxval=1.), 'relu'),\n",
    "    'random_Normal_initializers' : model(tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None), 'relu'),\n",
    "    \n",
    "}\n",
    "print('\\n\\n\\n')\n",
    "for i,j in initializers.items():\n",
    "    print(f'{i} : {j}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55/55 [==============================] - 2s 15ms/step - loss: 0.5935 - accuracy: 0.8331 - val_loss: 0.2837 - val_accuracy: 0.9194\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.2688 - accuracy: 0.9237 - val_loss: 0.2221 - val_accuracy: 0.9366\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.2163 - accuracy: 0.9390 - val_loss: 0.1864 - val_accuracy: 0.9456\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.1795 - accuracy: 0.9488 - val_loss: 0.1604 - val_accuracy: 0.9544\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.1534 - accuracy: 0.9565 - val_loss: 0.1401 - val_accuracy: 0.9618\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.1316 - accuracy: 0.9626 - val_loss: 0.1252 - val_accuracy: 0.9640\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.1155 - accuracy: 0.9675 - val_loss: 0.1205 - val_accuracy: 0.9656\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.1017 - accuracy: 0.9717 - val_loss: 0.1099 - val_accuracy: 0.9664\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.0894 - accuracy: 0.9750 - val_loss: 0.1031 - val_accuracy: 0.9708\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.0794 - accuracy: 0.9782 - val_loss: 0.0953 - val_accuracy: 0.9712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55/55 [==============================] - 2s 19ms/step - loss: 0.5778 - accuracy: 0.8357 - val_loss: 0.2772 - val_accuracy: 0.9196\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.2625 - accuracy: 0.9251 - val_loss: 0.2168 - val_accuracy: 0.9380\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.2108 - accuracy: 0.9399 - val_loss: 0.1855 - val_accuracy: 0.9502\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.1760 - accuracy: 0.9498 - val_loss: 0.1582 - val_accuracy: 0.9554\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.1507 - accuracy: 0.9570 - val_loss: 0.1461 - val_accuracy: 0.9562\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.1300 - accuracy: 0.9630 - val_loss: 0.1310 - val_accuracy: 0.9614\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.1129 - accuracy: 0.9677 - val_loss: 0.1180 - val_accuracy: 0.9664\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.0998 - accuracy: 0.9718 - val_loss: 0.1080 - val_accuracy: 0.9682\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.0873 - accuracy: 0.9756 - val_loss: 0.1003 - val_accuracy: 0.9704\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.0771 - accuracy: 0.9781 - val_loss: 0.0958 - val_accuracy: 0.9706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer HeNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.6474 - accuracy: 0.8177 - val_loss: 0.2403 - val_accuracy: 0.9308\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.2151 - accuracy: 0.9388 - val_loss: 0.1716 - val_accuracy: 0.9528\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.1554 - accuracy: 0.9559 - val_loss: 0.1378 - val_accuracy: 0.9618\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.1208 - accuracy: 0.9659 - val_loss: 0.1156 - val_accuracy: 0.9688\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.0955 - accuracy: 0.9735 - val_loss: 0.1046 - val_accuracy: 0.9716\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.0811 - accuracy: 0.9765 - val_loss: 0.0939 - val_accuracy: 0.9722\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.0662 - accuracy: 0.9814 - val_loss: 0.0927 - val_accuracy: 0.9730\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.0556 - accuracy: 0.9847 - val_loss: 0.0816 - val_accuracy: 0.9760\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.0468 - accuracy: 0.9871 - val_loss: 0.0790 - val_accuracy: 0.9760\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.0388 - accuracy: 0.9897 - val_loss: 0.0744 - val_accuracy: 0.9772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer HeUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55/55 [==============================] - 1s 14ms/step - loss: 0.6285 - accuracy: 0.8282 - val_loss: 0.2425 - val_accuracy: 0.9314\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.2126 - accuracy: 0.9392 - val_loss: 0.1664 - val_accuracy: 0.9542\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.1515 - accuracy: 0.9570 - val_loss: 0.1355 - val_accuracy: 0.9622\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.1181 - accuracy: 0.9665 - val_loss: 0.1113 - val_accuracy: 0.9690\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 2s 35ms/step - loss: 0.0947 - accuracy: 0.9730 - val_loss: 0.1006 - val_accuracy: 0.9702\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 3s 53ms/step - loss: 0.0762 - accuracy: 0.9787 - val_loss: 0.0883 - val_accuracy: 0.9722\n",
      "Epoch 7/10\n",
      "13/55 [======>.......................] - ETA: 4:23 - loss: 0.0595 - accuracy: 0.9830"
     ]
    }
   ],
   "source": [
    "\n",
    "initializers = {\n",
    "    'Xavier_Normal_initializers' : model(tf.keras.initializers.GlorotNormal(seed=None), 'tanh'),\n",
    "    'Xavier_Uniform_initializers' : model(tf.keras.initializers.GlorotUniform(seed=None), 'tanh'),\n",
    "    'He_Normal_initializers' : model(tf.keras.initializers.HeNormal(), 'relu'),\n",
    "    'He_Uniform_initializers' : model(tf.keras.initializers.HeUniform(), 'relu')\n",
    "    \n",
    "}\n",
    "print('\\n\\n\\n')\n",
    "for i,j in initializers.items():\n",
    "    print(f'{i} : {j}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we get the output as:\n",
    "- zero_initializers : 0.1125\n",
    "- random_Uniform_initializers : 0.8177\n",
    "- random_Normal_initializers : 0.97740\n",
    "- Xavier_Normal_initializers : 0.9782 \n",
    "- Xavier_Uniform_initializers : 0.9781 \n",
    "- He_Normal_initializers : 0.9897 \n",
    "- He_Uniform_initializers : 0.9830"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Discuss the considerations and tradeoffs when choosing the appropriate weight initialization technique for a given neural network architecture and task.\n",
    "\n",
    "Ans: When choosing a weight initialization technique for a neural network, consider the architecture and task. \n",
    "- Xavier/Glorot initialization is good for tanh and sigmoid activations, while He initialization suits ReLU-based activations. \n",
    "- Proper initialization helps prevent vanishing/exploding gradients. Experiment with different techniques to find what works best for your specific model and problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
