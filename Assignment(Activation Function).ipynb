{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5a94dd9-0055-43c5-927a-e3e5f5eadca6",
   "metadata": {},
   "source": [
    "#### Question 1-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ec9bd1-0de9-44a3-bab5-28ac667daa87",
   "metadata": {},
   "source": [
    "The activation function decides whether a neuron should be activated or not by calculating the weighted sum and further adding bias to it. The purpose of the activation function is to introduce non-linearity into the output of a neuron. \n",
    "\n",
    "A neural network without an activation function is essentially just a linear regression model. The activation function does the non-linear transformation to the input making it capable to learn and perform more complex tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c17ba0-279b-4df2-9770-f33c5cbb400b",
   "metadata": {},
   "source": [
    "#### Question 2-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e81d9a4-ad04-4e04-af62-654fdd07a6ab",
   "metadata": {},
   "source": [
    "Some common types of activatiopn functions used in Neural Networks are:\n",
    "1. Sigmoid \n",
    "2. Tangent hyperbolic\n",
    "3. Relu\n",
    "4. Leaky Relu\n",
    "5. Softmax\n",
    "6. Exponential Linear Unit\n",
    "7. Scaled Elu\n",
    "8. Swish\n",
    "9. Maxout\n",
    "10. Softplus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ab0c03-64df-490c-b47a-b1d42f860c23",
   "metadata": {},
   "source": [
    "#### Question 3-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0929d9ed-e4b3-4ac3-aa1d-62400becd99d",
   "metadata": {},
   "source": [
    "Activation functions play an integral role in neural networks by introducing nonlinearity. This nonlinearity allows neural networks to develop complex representations and functions based on the inputs that would not be possible with a simple linear regression model.\n",
    "\n",
    "Further , it helps to convert the cummulated weights from different neurons of the previous layers into a specified range, depending upon the type of AF(Activation Function) used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c8e56a-56ae-4991-9274-62e4e7f4deff",
   "metadata": {},
   "source": [
    "#### Question 4-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f782eea2-f4df-4090-b95b-479ec7937afc",
   "metadata": {},
   "source": [
    "SIGMOID ACTIVATION FUNCTION-\n",
    "Usually used in output layer of a binary classification, where result is either 0 or 1, as value for sigmoid function lies between 0 and 1 only so, result can be predicted easily to be 1 if value is greater than 0.5 and 0 otherwise.\n",
    "\n",
    "Advantages-\n",
    "- It gives smooth gradient, thereby, preventing jumps in output values.\n",
    "- With 1 and 0, it makes a clear prediction, just like Probability.\n",
    "- Another advantage of this function is that when used with (- infinite, + infinite) as in the linear function, it returns a value in the range of (0,1). As a result, the activation value does not disappear.\n",
    "\n",
    "Disadvantages-\n",
    "- Vanishing Gradient problem\n",
    "- Not a zero centric function\n",
    "- It is computationally expensive function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a7ec8-351d-4f57-be79-5e4b0586d133",
   "metadata": {},
   "source": [
    "#### Question 5-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c419de-3052-4955-8032-19bece1b456d",
   "metadata": {},
   "source": [
    "he rectified linear activation function or ReLU is a non-linear function or piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.\n",
    "It is the most commonly used activation function in neural networks, especially in Convolutional Neural Networks (CNNs) & Multilayer perceptrons.\n",
    "\n",
    "It is simple yet it is more effective than it's predecessors like sigmoid or tanh.\n",
    "\n",
    "- ReLU is used in the hidden layers instead of Sigmoid or tanh as using sigmoid or tanh in the hidden layers leads to the problem of \"Vanishing Gradient\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0f3c5b-3320-4753-870a-17fbae6e225b",
   "metadata": {},
   "source": [
    "#### Question 6-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6ff3a6-8e69-42e6-8336-aa523a41a140",
   "metadata": {},
   "source": [
    "ReLU is used in the hidden layers instead of Sigmoid or tanh as using sigmoid or tanh in the hidden layers leads to the problem of \"Vanishing Gradient\".The \"Vanishing Gradient\" prevents the earlier layers from learning important information when the network is backpropagating. The sigmoid which is a logistic function is more preferrable to be used in regression or binary classification related problems and that too only in the output layer, as the output of a sigmoid function ranges from 0 to 1. Also Sigmoid and tanh saturate and have lesser sensitivity.\n",
    "\n",
    "- Simpler Computation: Derivative remains constant i.e 1 for a positive input and thus reduces the time taken for the model to learn and in minimizing the errors.\n",
    "- Representational Sparsity: It is capable of outputting a true zero value.\n",
    "- Linearity: Linear activation functions are easier to optimize and allow for a smooth flow. So, it is best suited for supervised tasks on large sets of labelled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45355864-7ea5-4d36-8e7a-0065c654c808",
   "metadata": {},
   "source": [
    "#### Question 7-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7031f1-48d9-468b-b3e2-eeacf7cd72be",
   "metadata": {},
   "source": [
    "Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. The slope coefficient is determined before training, i.e. it is not learnt during training. This type of activation function is popular in tasks where we may suffer from sparse gradients, for example training generative adversarial networks.\n",
    "As for the ReLU activation function, the gradient is 0 for all the values of inputs that are less than zero, which would deactivate the neurons in that region and may cause dying ReLU problem. Leaky ReLU is defined to address this problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c469e0f-e1a1-4e0f-9b4d-6cd236fd5961",
   "metadata": {},
   "source": [
    "#### Question 8-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ee9d3-bbc4-434a-9880-03996be32d2f",
   "metadata": {},
   "source": [
    "Softmax is a mathematical function that converts a vector of numbers into a vector of probabilities, where the probabilities of each value are proportional to the relative scale of each value in the vector.\n",
    "\n",
    "The most common use of the softmax function in applied machine learning is in its use as an activation function in a neural network model. Specifically, the network is configured to output N values, one for each class in the classification task, and the softmax function is used to normalize the outputs, converting them from weighted sum values into probabilities that sum to one. Each value in the output of the softmax function is interpreted as the probability of membership for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a4237b-df94-4395-a59e-f51417091827",
   "metadata": {},
   "source": [
    "#### Question 9-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62395d7-229f-4188-92ae-c9308ce98db5",
   "metadata": {},
   "source": [
    "Another activation function that is common in deep learning is the tangent hyperbolic function simply referred to as tanh function.\n",
    "We observe that the tanh function is a shifted and stretched version of the sigmoid.\n",
    "The output range of the tanh function is (-1, 1) and presents a similar behavior with the sigmoid function. The main difference is the fact that the tanh function pushes the input values to 1 and -1 instead of 1 and 0.\n",
    "- The tanh function is a stretched and shifted version of the sigmoid. Therefore, there are a lot of similarities.\n",
    "- Both functions belong to the S-like functions that suppress the input value to a bounded range. This helps the network to keep its weights bounded and prevents the exploding gradient problem where the value of the gradients becomes very large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90acf17a-115b-434e-872f-2662fb6fcd08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
