{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50be9385-d67e-4ddf-8952-666d588e1922",
   "metadata": {},
   "source": [
    "### Part 1- Understanding Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3485f87f-5f6f-4698-b066-7db87901fa53",
   "metadata": {},
   "source": [
    "1. What is the role of optimization algorithms in artificial neural networks? Why are they necessary?\n",
    "\n",
    "Ans: Choosing the right optimizer for a neural network involves tradeoffs among convergence speed, stability, and generalization performance. \n",
    "        Gradient Descent is simple but slow. Stochastic Gradient Descent (SGD) converges faster but may be unstable. \n",
    "        Adam optimizer balances speed and stability with adaptive learning rates. RMSprop is stable but might converge slower. \n",
    "The choice depends on the specific task, architecture, and data. Experimentation and tuning are essential to find the best optimizer for optimal training performance.\n",
    "\n",
    "2. Explainthe concept of gradient descent and its variants. Discuss their differences and tradeoffs in terms of convergence speed and memory requirements.\n",
    "\n",
    "Ans: Gradient descent is an optimization technique used in neural networks to minimize errors during training. \n",
    "        It calculates the gradient (direction of steepest ascent) and updates model weights to reach the minimum error. \n",
    "        Variants like Stochastic GD use random samples for faster convergence but higher memory, while Mini-batch GD strikes a balance by using batches of data. \n",
    "        Each variant has tradeoffs in terms of convergence speed and memory requirements.\n",
    "\n",
    "3. Describe the challenges associated with traditional gradient descent optimization methods (e.g., slow convergence, local minima). How do modern optimizers address these challenges?\n",
    "\n",
    "Ans: Traditional gradient descent methods suffer from slow convergence and getting stuck in local minima. Modern optimizers address these challenges by using \n",
    "        adaptive learning rates (e.g., Adam) that speed up convergence by adjusting step sizes for each parameter. \n",
    "        They also use momentum to escape local minima and converge faster towards the global minimum.\n",
    "\n",
    "4. Discuss the concepts of momentum and learning rate int the context of optimization algorithms. How do they impact convergence and model perfromance?\n",
    "\n",
    "Ans: In optimization algorithms, momentum is a technique that helps the optimizer move faster towards the minimum by adding a fraction of the previous gradient. \n",
    "        Learning rate controls the step size in each update. Proper momentum prevents oscillations, while an appropriate learning rate balances convergence speed \n",
    "        and stable model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb8bd06-95ef-4566-8a6b-5b14a3859236",
   "metadata": {},
   "source": [
    "### Part 2: Optimizer Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859e12f2-c9d4-40de-a568-87484caed346",
   "metadata": {},
   "source": [
    "5. Explain the concept of Stochastic Gradient Descent (SGD) and its advantages compared to traditional gradient descent. Discuss its limitations ad scenarios where it is most suitable.\n",
    "\n",
    "Ans: Stochastic Gradient Descent (SGD) is an optimization method that updates model weights using one random training sample at a time instead of the entire dataset. \n",
    "        It converges faster, makes efficient use of memory, and escapes local minima better. However, its randomness may cause noisy updates, making it less suitable \n",
    "        for smoother loss landscapes or when noise is detrimental.\n",
    "\n",
    "6. Describe the concept of Adam optimizer and how it combines momentum and adaptive learning rates. Discuss its benefits and potential drawbacks.\n",
    "\n",
    "Ans: The Adam optimizer combines the benefits of momentum and adaptive learning rates. It calculates individual adaptive learning rates for each parameter based on \n",
    "        past gradients and squares of gradients. This helps it converge faster and be more robust to different learning rates. However, it may require tuning and \n",
    "        could overshoot the optimal values in some cases.\n",
    "\n",
    "7. Explain the concept of RMSprop optimizer and how it addresses the challenges of adaptive learning rates. Compare it with Adam and discuss their relative strengths and weaknesses. \n",
    "\n",
    "Ans: RMSprop is an optimizer that addresses the challenges of adaptive learning rates by using a moving average of squared gradients. It helps prevent oscillations \n",
    "        in training and converges faster. Compared to Adam, RMSprop is simpler and requires less memory, but it may not adapt learning rates as effectively. \n",
    "        Adam performs well in many scenarios but might need more tuning.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8827d2d-5ea2-476d-b481-f230599170c7",
   "metadata": {},
   "source": [
    "### Part 3- Applying Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6329a139-99e6-4542-bdba-e761aa3cace5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-26 14:41:36.650164: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 14:41:36.722246: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 14:41:36.724330: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-26 14:41:38.037269: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72613ac4-54a6-408e-84ba-b8fc768e2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense,Flatten,BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae55d0e1-7505-4887-a3ac-b44501cc6c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(OPTIMIZER):\n",
    "    # Loading the Dataset\n",
    "    (X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    #Scaling the data\n",
    "    X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "    y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "    \n",
    "    #Scaling the test data \n",
    "    X_test=X_test/255\n",
    "    \n",
    "    model=tf.keras.models.Sequential()\n",
    "    model.add(Flatten(input_shape=[28, 28], name=\"inputLayer\"))\n",
    "    model.add(Dense(200, activation=\"relu\", name=\"hiddenLayer1\"))\n",
    "    model.add(Dense(100, activation=\"relu\", name=\"hiddenLayer2\"))\n",
    "    model.add(Dense(10, activation=\"softmax\", name=\"outputLayer\"))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer=OPTIMIZER,\n",
    "                metrics=[\"accuracy\"])\n",
    "    model.fit(X_train, y_train, epochs=25,\n",
    "                    validation_data=(X_valid, y_valid), batch_size=1000)\n",
    "    \n",
    "    return model.history.history['val_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a89628-4f6a-4bba-8c35-4f7aa578fc26",
   "metadata": {},
   "source": [
    "8. Implement SGD, Adam, and RMSprop optimizers in a deep learning model using a framework of your choice. \n",
    "- Train the model on a suitable dataset and compare their impact on model convergence and performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5713fae-e3b2-428a-b349-944db754ea7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 2.1003 - accuracy: 0.3501 - val_loss: 1.9230 - val_accuracy: 0.5358\n",
      "Epoch 2/25\n",
      "55/55 [==============================] - 15s 283ms/step - loss: 1.7519 - accuracy: 0.6249 - val_loss: 1.5568 - val_accuracy: 0.6888\n",
      "Epoch 3/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 1.3971 - accuracy: 0.7265 - val_loss: 1.2202 - val_accuracy: 0.7556\n",
      "Epoch 4/25\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 1.1063 - accuracy: 0.7827 - val_loss: 0.9757 - val_accuracy: 0.7992\n",
      "Epoch 5/25\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.9057 - accuracy: 0.8128 - val_loss: 0.8147 - val_accuracy: 0.8240\n",
      "Epoch 6/25\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.7741 - accuracy: 0.8315 - val_loss: 0.7084 - val_accuracy: 0.8398\n",
      "Epoch 7/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.6855 - accuracy: 0.8433 - val_loss: 0.6347 - val_accuracy: 0.8504\n",
      "Epoch 8/25\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.6225 - accuracy: 0.8517 - val_loss: 0.5802 - val_accuracy: 0.8592\n",
      "Epoch 9/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.5758 - accuracy: 0.8590 - val_loss: 0.5388 - val_accuracy: 0.8656\n",
      "Epoch 10/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.5395 - accuracy: 0.8648 - val_loss: 0.5058 - val_accuracy: 0.8742\n",
      "Epoch 11/25\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.5107 - accuracy: 0.8697 - val_loss: 0.4792 - val_accuracy: 0.8800\n",
      "Epoch 12/25\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.4871 - accuracy: 0.8741 - val_loss: 0.4575 - val_accuracy: 0.8838\n",
      "Epoch 13/25\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.4675 - accuracy: 0.8778 - val_loss: 0.4392 - val_accuracy: 0.8872\n",
      "Epoch 14/25\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.4509 - accuracy: 0.8806 - val_loss: 0.4233 - val_accuracy: 0.8908\n",
      "Epoch 15/25\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.4368 - accuracy: 0.8840 - val_loss: 0.4100 - val_accuracy: 0.8930\n",
      "Epoch 16/25\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.4244 - accuracy: 0.8859 - val_loss: 0.3982 - val_accuracy: 0.8958\n",
      "Epoch 17/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.4135 - accuracy: 0.8881 - val_loss: 0.3881 - val_accuracy: 0.8966\n",
      "Epoch 18/25\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.4039 - accuracy: 0.8905 - val_loss: 0.3785 - val_accuracy: 0.8966\n",
      "Epoch 19/25\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.3952 - accuracy: 0.8924 - val_loss: 0.3702 - val_accuracy: 0.8982\n",
      "Epoch 20/25\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.3874 - accuracy: 0.8939 - val_loss: 0.3629 - val_accuracy: 0.9002\n",
      "Epoch 21/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.3803 - accuracy: 0.8953 - val_loss: 0.3559 - val_accuracy: 0.9012\n",
      "Epoch 22/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.3736 - accuracy: 0.8969 - val_loss: 0.3493 - val_accuracy: 0.9034\n",
      "Epoch 23/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.3676 - accuracy: 0.8979 - val_loss: 0.3435 - val_accuracy: 0.9048\n",
      "Epoch 24/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.3620 - accuracy: 0.8992 - val_loss: 0.3383 - val_accuracy: 0.9064\n",
      "Epoch 25/25\n",
      "55/55 [==============================] - 0s 8ms/step - loss: 0.3568 - accuracy: 0.9002 - val_loss: 0.3336 - val_accuracy: 0.9078\n",
      "Epoch 1/25\n",
      "55/55 [==============================] - 2s 16ms/step - loss: 0.7148 - accuracy: 0.8143 - val_loss: 0.2659 - val_accuracy: 0.9282\n",
      "Epoch 2/25\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.2396 - accuracy: 0.9320 - val_loss: 0.1878 - val_accuracy: 0.9476\n",
      "Epoch 3/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.1764 - accuracy: 0.9497 - val_loss: 0.1505 - val_accuracy: 0.9600\n",
      "Epoch 4/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.1411 - accuracy: 0.9601 - val_loss: 0.1255 - val_accuracy: 0.9638\n",
      "Epoch 5/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.1131 - accuracy: 0.9677 - val_loss: 0.1132 - val_accuracy: 0.9678\n",
      "Epoch 6/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.0940 - accuracy: 0.9727 - val_loss: 0.0986 - val_accuracy: 0.9708\n",
      "Epoch 7/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0801 - accuracy: 0.9769 - val_loss: 0.0903 - val_accuracy: 0.9738\n",
      "Epoch 8/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.0690 - accuracy: 0.9802 - val_loss: 0.0849 - val_accuracy: 0.9758\n",
      "Epoch 9/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0583 - accuracy: 0.9837 - val_loss: 0.0766 - val_accuracy: 0.9778\n",
      "Epoch 10/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.0505 - accuracy: 0.9860 - val_loss: 0.0753 - val_accuracy: 0.9784\n",
      "Epoch 11/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0437 - accuracy: 0.9883 - val_loss: 0.0747 - val_accuracy: 0.9788\n",
      "Epoch 12/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0383 - accuracy: 0.9894 - val_loss: 0.0701 - val_accuracy: 0.9792\n",
      "Epoch 13/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.0340 - accuracy: 0.9909 - val_loss: 0.0690 - val_accuracy: 0.9802\n",
      "Epoch 14/25\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.0290 - accuracy: 0.9924 - val_loss: 0.0703 - val_accuracy: 0.9790\n",
      "Epoch 15/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0264 - accuracy: 0.9936 - val_loss: 0.0660 - val_accuracy: 0.9796\n",
      "Epoch 16/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0228 - accuracy: 0.9944 - val_loss: 0.0678 - val_accuracy: 0.9794\n",
      "Epoch 17/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0194 - accuracy: 0.9958 - val_loss: 0.0700 - val_accuracy: 0.9802\n",
      "Epoch 18/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0183 - accuracy: 0.9959 - val_loss: 0.0721 - val_accuracy: 0.9776\n",
      "Epoch 19/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0161 - accuracy: 0.9964 - val_loss: 0.0695 - val_accuracy: 0.9800\n",
      "Epoch 20/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.0131 - accuracy: 0.9977 - val_loss: 0.0676 - val_accuracy: 0.9812\n",
      "Epoch 21/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.0116 - accuracy: 0.9982 - val_loss: 0.0675 - val_accuracy: 0.9820\n",
      "Epoch 22/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.0102 - accuracy: 0.9985 - val_loss: 0.0669 - val_accuracy: 0.9806\n",
      "Epoch 23/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0089 - accuracy: 0.9988 - val_loss: 0.0737 - val_accuracy: 0.9794\n",
      "Epoch 24/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.0083 - accuracy: 0.9989 - val_loss: 0.0714 - val_accuracy: 0.9802\n",
      "Epoch 25/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0075 - accuracy: 0.9992 - val_loss: 0.0703 - val_accuracy: 0.9814\n",
      "Epoch 1/25\n",
      "55/55 [==============================] - 1s 12ms/step - loss: 0.6295 - accuracy: 0.8276 - val_loss: 0.3009 - val_accuracy: 0.9184\n",
      "Epoch 2/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.2760 - accuracy: 0.9203 - val_loss: 0.2005 - val_accuracy: 0.9446\n",
      "Epoch 3/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.2071 - accuracy: 0.9392 - val_loss: 0.1918 - val_accuracy: 0.9438\n",
      "Epoch 4/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.1664 - accuracy: 0.9511 - val_loss: 0.1331 - val_accuracy: 0.9630\n",
      "Epoch 5/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.1380 - accuracy: 0.9593 - val_loss: 0.1193 - val_accuracy: 0.9650\n",
      "Epoch 6/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.1161 - accuracy: 0.9651 - val_loss: 0.1043 - val_accuracy: 0.9700\n",
      "Epoch 7/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.1005 - accuracy: 0.9698 - val_loss: 0.1005 - val_accuracy: 0.9696\n",
      "Epoch 8/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.0863 - accuracy: 0.9743 - val_loss: 0.0905 - val_accuracy: 0.9722\n",
      "Epoch 9/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0763 - accuracy: 0.9770 - val_loss: 0.0823 - val_accuracy: 0.9766\n",
      "Epoch 10/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.0682 - accuracy: 0.9797 - val_loss: 0.0884 - val_accuracy: 0.9730\n",
      "Epoch 11/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0606 - accuracy: 0.9821 - val_loss: 0.0810 - val_accuracy: 0.9760\n",
      "Epoch 12/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.0540 - accuracy: 0.9841 - val_loss: 0.0765 - val_accuracy: 0.9788\n",
      "Epoch 13/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0483 - accuracy: 0.9848 - val_loss: 0.0709 - val_accuracy: 0.9778\n",
      "Epoch 14/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.0425 - accuracy: 0.9875 - val_loss: 0.0667 - val_accuracy: 0.9798\n",
      "Epoch 15/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0389 - accuracy: 0.9885 - val_loss: 0.0748 - val_accuracy: 0.9760\n",
      "Epoch 16/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0341 - accuracy: 0.9903 - val_loss: 0.0682 - val_accuracy: 0.9796\n",
      "Epoch 17/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0324 - accuracy: 0.9903 - val_loss: 0.0685 - val_accuracy: 0.9810\n",
      "Epoch 18/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0272 - accuracy: 0.9924 - val_loss: 0.0768 - val_accuracy: 0.9770\n",
      "Epoch 19/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0251 - accuracy: 0.9935 - val_loss: 0.0716 - val_accuracy: 0.9784\n",
      "Epoch 20/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0218 - accuracy: 0.9940 - val_loss: 0.0677 - val_accuracy: 0.9788\n",
      "Epoch 21/25\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.0202 - accuracy: 0.9948 - val_loss: 0.0913 - val_accuracy: 0.9718\n",
      "Epoch 22/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0197 - accuracy: 0.9948 - val_loss: 0.0752 - val_accuracy: 0.9786\n",
      "Epoch 23/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0161 - accuracy: 0.9960 - val_loss: 0.0665 - val_accuracy: 0.9818\n",
      "Epoch 24/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0142 - accuracy: 0.9969 - val_loss: 0.0680 - val_accuracy: 0.9804\n",
      "Epoch 25/25\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.0125 - accuracy: 0.9970 - val_loss: 0.0799 - val_accuracy: 0.9786\n",
      "\n",
      "\n",
      "\n",
      "{'score_SGD': 0.907800018787384, 'score_Adam': 0.9814000129699707, 'score_RMSProp': 0.978600025177002}\n"
     ]
    }
   ],
   "source": [
    "score_data={\n",
    "    'score_SGD':model('SGD'),\n",
    "    'score_Adam':model('adam'),\n",
    "    'score_RMSProp':model('RMSProp')\n",
    "}\n",
    "print(\"\\n\\n\")\n",
    "print(score_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c8c8d3-84d2-4216-8148-d9654bcfae74",
   "metadata": {},
   "source": [
    "9. Discuss the considerations and tradeoffs when choosing the appropriate optimizer for a given neural network architecture and task. Consider factors such as convergence speed, stability and generalization performance. \n",
    "\n",
    "Ans: Choosing the right optimizer for a neural network involves tradeoffs among convergence speed, stability, and generalization performance. \n",
    "- Gradient Descent is simple but slow. \n",
    "- Stochastic Gradient Descent (SGD) converges faster but may be unstable. \n",
    "- Adam optimizer balances speed and stability with adaptive learning rates. \n",
    "- RMSprop is stable but might converge slower. \n",
    "\n",
    "The choice depends on the specific task, architecture, and data. Experimentation and tuning are essential to find the best optimizer for optimal trai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd46d6b-5f4f-41f1-b81b-bf1120afa85f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
