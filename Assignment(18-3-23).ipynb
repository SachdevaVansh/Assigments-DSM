{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "538e52c2-459c-4aed-abeb-53a7843288b4",
   "metadata": {},
   "source": [
    "#### Question 1-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e754a8-17d4-49ee-93f3-54120b567793",
   "metadata": {},
   "source": [
    "The filter method ranks each feature based on some uni-variate metric and then selects the highest-ranking features. Some of the uni-variate metrics are-\n",
    "- variance: removing constant and quasi constant features\n",
    "- chi-square: used for classification. It is a statistical test of independence to determine the dependency of two variables.\n",
    "- correlation coefficients: removes duplicate features\n",
    "- Information gain or mutual information: assess the dependency of the independent variable in predicting the target variable. In other words, it determines the ability of the independent feature to predict the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80d4046-5eab-4ba9-93f2-6aacc70371ad",
   "metadata": {},
   "source": [
    "Working--->\n",
    "\n",
    "It includes the selection of independent features with\n",
    "\n",
    "- High correlation with the target variable\n",
    "- Low correlation with another independent variable\n",
    "- Higher information gain or mutual information of the independent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa686d9c-add3-4e4d-9439-f8eb5fcb1a61",
   "metadata": {},
   "source": [
    "#### Question 2-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607cd4f0-8a67-41f2-aab1-ef0dc7d8d984",
   "metadata": {},
   "source": [
    "#### Filter Method-\n",
    "- uses proxy measures to select the important features\n",
    "- computationally faster\n",
    "- avoids overfitting\n",
    "- sometimes may fail to select the best features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aebb99-c5f4-4be7-8fb9-2ac29d6d0f2b",
   "metadata": {},
   "source": [
    "#### Wrapper Method-\n",
    "- Uses predictive models for feature selection\n",
    "- computationally expensive and slower\n",
    "- prone to overfitting\n",
    "- Best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b56627-29d8-4969-a09c-aab9f60cea77",
   "metadata": {},
   "source": [
    "#### Question 3-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9d1b15-ed91-445e-a2e5-fad274b0d4be",
   "metadata": {},
   "source": [
    "Some common techniques used in Embedded Feature Selection Method are-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc7c5ba-7c41-46f7-89b3-c11483d1c238",
   "metadata": {},
   "source": [
    "- Regularization – This method adds a penalty to different parameters of the machine learning model to avoid over-fitting of the model. This approach of feature selection uses Lasso (L1 regularization) and Elastic nets (L1 and L2 regularization). The penalty is applied over the coefficients, thus bringing down some coefficients to zero. The features having zero coefficient can be removed from the dataset.\n",
    "- Tree-based methods – These methods such as Random Forest, Gradient Boosting provides us feature importance as a way to select features as well. Feature importance tells us which features are more important in making an impact on the target feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d97632b-0633-4b4b-b96e-a847f4e193f2",
   "metadata": {},
   "source": [
    "#### Question 4-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3ff7f9-025a-4cd2-8f9e-1b5a730cd302",
   "metadata": {},
   "source": [
    "The disadvantage of Filter methods-\n",
    "\n",
    "- The filter method looks at individual features for identifying it’s relative importance. A feature may not be useful on its own but maybe an important influencer when combined with other features. Filter methods may miss such features.\n",
    "- does not considers the interactions among the input features.\n",
    "- does not remove multicollinearity\n",
    "- sometimes may fail in selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a44e48-a0a4-4b32-9e38-c0a9e4483b7e",
   "metadata": {},
   "source": [
    "#### Question 5-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c773c9-dd2f-4250-864c-cf4e962fde9e",
   "metadata": {},
   "source": [
    "- Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it. Filter methods are much faster compared to wrapper methods as they do not involve training the models.\n",
    "- Filter methods can be used when the process is comutationally expensive \n",
    "- Dependent on model selection is not required\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e8af4-d08b-4f98-849c-a604c91579db",
   "metadata": {},
   "source": [
    "#### Question 6-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e76859-0b25-4eff-83ee-e3b67439c81d",
   "metadata": {},
   "source": [
    "1. Identify input features having high correlation with target variable.\n",
    "    - We want to keep features with only a high correlation with the target variable. This implies that the input feature has a high influence in predicting the target variable.\n",
    "\n",
    "    - We set the threshold to the absolute value of 0.4(say). We keep input features only if the correlation of the input feature with the target variable is greater than 0.4\n",
    "\n",
    "2. Identify input features that have a low correlation with other independent variables.\n",
    "    - Iterating through all the filtered input features based on step 1 and checking each input feature correlation with all other input features.\n",
    "\n",
    "    - We will keep input features that are not highly correlated with other input features.\n",
    "\n",
    "3. Find the information gain or mutual information of the independent variable with respect to a target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c636a9aa-aa2f-4dcb-806d-beff26e5a030",
   "metadata": {},
   "source": [
    "#### Question 7-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b9a506-75df-4b83-b757-b7d8e6445204",
   "metadata": {},
   "source": [
    "##### Procedure to be followed- \n",
    "1. Train a Machine learning Algorithm.\n",
    "2. Derive the feature importance.\n",
    "3. Remove non-important features.\n",
    "\n",
    "##### We can use the following Embedded Methods Techniques:\n",
    "\n",
    "1. Lasso Regularization\n",
    "2. Trees - Decision Trees and Random Forest\n",
    "\n",
    "\n",
    "##### Regularization \n",
    "consists in adding a penalty on the different parameters of the model to reduce the freedom of the model.\n",
    "\n",
    "Three Types of regularization:\n",
    "\n",
    "- L1 regularization (Lasso)\n",
    "- L2 regularization (Ridge)\n",
    "- L1/L2 regularization (Elastic Net)\n",
    "\n",
    "##### Decision Trees\n",
    "\n",
    "- Most popular Machine learning algorithms\n",
    "- Highly accurate\n",
    "- Good generalisation (low overfitting)\n",
    "- Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2746579d-72ca-463d-802f-aa764773060c",
   "metadata": {},
   "source": [
    "#### Question 8-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb03634-5efb-4fef-b39d-2d81c523249f",
   "metadata": {},
   "source": [
    "The following points can be followed to use the Wrapper Method to slect the best features : \n",
    "\n",
    "- Use predictive machine learning models to score the feature subset.\n",
    "- Train a new model on each feature subset.\n",
    "\n",
    "After model is selected, the below techniques can be used to slect the important features:\n",
    "1. Forward Feature Selection, add 1 feature at a time\n",
    "2. Backward Feature Elimination, removes 1 feature at a time.\n",
    "3. Exhaustive Feature Search, searches across all possible feature combinations.(Ex-Grid Search, Randomized Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291477c6-c067-4e72-b5b5-42f6a155e9a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
