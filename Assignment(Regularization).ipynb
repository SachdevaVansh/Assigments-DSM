{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Understanding Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is regularization in the context of deep learning? Why is it important?\n",
    "\n",
    "Ans: Regularization in deep learning refers to techniques that prevent overfitting by adding extra constraints or penalties to the model during training. \n",
    "It is essential because it helps improve generalization, reducing the risk of the model memorizing the training data and performing poorly on unseen data.\n",
    " \n",
    "2. Explain the bias-variance tradeoff and how regularization helps in addressing this tradeoff.\n",
    "\n",
    "Ans: The bias-variance tradeoff refers to the balance between a model's ability to fit training data (low bias) and generalize to unseen data (low variance).\n",
    "Regularization helps address this tradeoff by adding penalties to the model's complexity during training. It reduces overfitting (high variance) by favoring simpler models (higher bias) that generalize better.\n",
    "\n",
    "3. Describe the concept of L1 and L2 regularization. How do they differ in terms of penalty calculation and their effects on the model? \n",
    "\n",
    "Ans:  \n",
    "- L1 regularization adds a penalty proportional to the absolute value of weights, promoting sparsity and feature Selection. \n",
    "- L2 regularization adds a penalty proportional to the square of weights, encouraging smaller but non-zero weights. \n",
    "- L1 can lead to more sparse models, while L2 encourages smaller weights without making them exactly zero.\n",
    "\n",
    "4. Discuss the role of regularization in preventing overfitting and improving the generalization  of deep learning models. \n",
    "\n",
    "Ans: Regularization prevents overfitting in deep learning models by adding constraints or penalties during training. \n",
    "It discourages the model from becoming too complex and memorizing the training data. By favoring simpler models, regularization improves generalization, allowing the model to perform better on unseen data and make more accurate predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Regularization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Explain Dropout regularization and how it works to reduce overfitting. Discuss the impact of Dropout on model training and inference. \n",
    "Ans: Dropout regularization randomly deactivates (sets to zero) some neurons during training. This prevents neurons from relying too much on others, reducing overfitting.\n",
    "During inference, neurons are activated with scaled weights to compensate for the dropout. Dropout can slow training, but it helps improve generalization and model performance.\n",
    "\n",
    "6. Describe the concept of Early Stopping as a form of regularization. How does it help prevent overfitting during the training process? \n",
    "\n",
    "Ans: Early stopping is a form of regularization that stops training when the model's performance on a validation set starts to degrade. By preventing the model from training for too long, it avoids overfitting. It ensures the model retains its ability to generalize well and perform better on unseen data.\n",
    "\n",
    "7. Explain the concept of Batch Normalization and its role as a form of regularization. How does Batch Normalization help in preventing overfitting? \n",
    "\n",
    "Ans: Batch Normalization is a technique that normalizes the activations in each layer of a neural network during training. \n",
    "It helps prevent overfitting by reducing internal covariate shift, making training more stable. This regularization improves generalization and allows for faster convergence during training.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Applying Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-26 14:58:05.301038: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 14:58:05.743549: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-26 14:58:05.745962: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-26 14:58:07.625959: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "\n",
    "\n",
    "def model(dropout_rate):\n",
    "    # loading dataset\n",
    "    (X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range\n",
    "    X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
    "    y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "    # scale the test set as well\n",
    "    X_test = X_test / 255.\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(Flatten(input_shape=[28, 28], name=\"inputLayer\"))\n",
    "    model.add(Dense(200, activation='relu', name=\"hiddenLayer1\"))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(100, activation='relu', name=\"hiddenLayer2\"))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(10, activation=\"softmax\", name=\"outputLayer\"))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=[\"accuracy\"])\n",
    "    model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid), batch_size=1000)\n",
    "    \n",
    "    return max(model.history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "55/55 [==============================] - 2s 15ms/step - loss: 0.8230 - accuracy: 0.7539 - val_loss: 0.2894 - val_accuracy: 0.9158\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.3148 - accuracy: 0.9075 - val_loss: 0.1935 - val_accuracy: 0.9448\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.2336 - accuracy: 0.9317 - val_loss: 0.1560 - val_accuracy: 0.9558\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.1907 - accuracy: 0.9435 - val_loss: 0.1288 - val_accuracy: 0.9618\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.1621 - accuracy: 0.9514 - val_loss: 0.1136 - val_accuracy: 0.9656\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.1402 - accuracy: 0.9584 - val_loss: 0.1017 - val_accuracy: 0.9700\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.1224 - accuracy: 0.9637 - val_loss: 0.0925 - val_accuracy: 0.9738\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.1117 - accuracy: 0.9669 - val_loss: 0.0876 - val_accuracy: 0.9748\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.1005 - accuracy: 0.9696 - val_loss: 0.0791 - val_accuracy: 0.9772\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.0888 - accuracy: 0.9733 - val_loss: 0.0774 - val_accuracy: 0.9768\n",
      "Epoch 1/10\n",
      "55/55 [==============================] - 1s 13ms/step - loss: 0.7279 - accuracy: 0.8046 - val_loss: 0.2784 - val_accuracy: 0.9198\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.2463 - accuracy: 0.9299 - val_loss: 0.1976 - val_accuracy: 0.9432\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 1s 15ms/step - loss: 0.1851 - accuracy: 0.9469 - val_loss: 0.1588 - val_accuracy: 0.9558\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 0s 9ms/step - loss: 0.1467 - accuracy: 0.9585 - val_loss: 0.1326 - val_accuracy: 0.9642\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.1190 - accuracy: 0.9666 - val_loss: 0.1143 - val_accuracy: 0.9674\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.1001 - accuracy: 0.9716 - val_loss: 0.1109 - val_accuracy: 0.9672\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.0863 - accuracy: 0.9756 - val_loss: 0.1005 - val_accuracy: 0.9716\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 1s 11ms/step - loss: 0.0737 - accuracy: 0.9793 - val_loss: 0.0942 - val_accuracy: 0.9724\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 1s 9ms/step - loss: 0.0637 - accuracy: 0.9818 - val_loss: 0.0842 - val_accuracy: 0.9748\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 1s 10ms/step - loss: 0.0557 - accuracy: 0.9846 - val_loss: 0.0807 - val_accuracy: 0.9758\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model Performance With Dropout Layer : 0.9771999716758728\n",
      "Model Performance Without Dropout Layer : 0.9757999777793884\n"
     ]
    }
   ],
   "source": [
    "# 8. Implement Dropout regularization in a deep learning model using a framework of your choice. Evaluate its impact on model performance and compare it with a model without Dropout. \n",
    "\n",
    "model_performance_with_Dropout_Layer = model(dropout_rate=0.2)\n",
    "model_performance_without_Dropout_Layer = model(dropout_rate=0)\n",
    "print(\"\\n\\n\\n\")\n",
    "print(f\"Model Performance With Dropout Layer : {model_performance_with_Dropout_Layer}\")\n",
    "print(f\"Model Performance Without Dropout Layer : {model_performance_without_Dropout_Layer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see in above out that using Dropout Layer out model performance increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Discuss the considerations and tradeoffs when choosing the appropriate regularization technique for a given deep learning task.\n",
    "\n",
    "Ans: When choosing a regularization technique, consider the task complexity and data size. L1 and L2 regularization penalize large weights, while Dropout deactivates neurons. Each technique affects model training and inference differently. \n",
    "Experimentation is key to finding the most suitable regularization method, balancing performance and computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
