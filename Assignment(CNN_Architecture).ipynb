{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b66075-9418-4d0b-9e3d-8c6540359923",
   "metadata": {},
   "source": [
    "# TOPIC: Understanding Pooling and Padding in CNN\n",
    "\n",
    "### 1. Describe the purpose and benefits of pooling in CNN.\n",
    "-     Ans: Pooling in CNN (Convolutional Neural Network) simplifies and compresses information by reducing the dimensions of feature maps, \n",
    "         which helps in retaining essential features and reducing computational load. It enhances translation invariance, aids in noise reduction, \n",
    "         and speeds up training while maintaining crucial image characteristics.\n",
    "\n",
    "### 2. Explain the difference between min pooling and max pooling.\n",
    "-     Ans: Min pooling and max pooling are types of pooling in CNN. Min pooling selects the smallest value in a pool, highlighting darker features. \n",
    "         Max pooling picks the largest value, emphasizing brighter features. Both reduce dimensionality and capture essential patterns, \n",
    "         but max pooling is more common for preserving prominent features.\n",
    "\n",
    "### 3. Discuss the concept of padding in CNN and its significance.\n",
    "-     Ans: Padding in CNN involves adding extra border pixels to input images before convolution. It prevents reduction in feature map size after convolution,\n",
    "         preserving information at image edges. Padding maintains spatial dimensions, aids feature extraction, and enables deeper networks by retaining \n",
    "         essential data during convolutions.\n",
    "\n",
    "### 4. Compare and contrast zero-padding and valid-padding in terms of their effects on the output feature map size.\n",
    "-     Ans: Zero-padding adds extra pixels around the input in CNN, maintaining feature map size after convolutions. \n",
    "         Valid-padding omits extra pixels, leading to smaller feature maps. Zero-padding helps retain spatial dimensions, \n",
    "         while valid-padding reduces them, impacting the output's spatial extent in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797bf9cf-3adb-4453-8958-d2c9b96237bd",
   "metadata": {},
   "source": [
    "# TOPIC: Exploring LeNet\n",
    "\n",
    "### 1. Provide a brief overview of LeNet-5 architecture.\n",
    "-   Ans: LeNet-5 is an early and influential Convolutional Neural Network (CNN) architecture. It consists of input, convolutional, pooling, and fully connected layers.\n",
    "         LeNet-5 was designed for handwritten digit recognition. It features progressively reduced spatial dimensions through convolutions and pooling, \n",
    "         followed by flattening and fully connected layers for classification. LeNet-5's architecture serves as a foundation for modern CNNs, \n",
    "         contributing to their development and success in image-related tasks.\n",
    "\n",
    "### 2. Describe the key components of LeNet-5 and their respective purposes.\n",
    "-   Ans: LeNet-5 has three main components: convolutional layers, pooling layers, and fully connected layers. \n",
    "         Convolutional layers extract features from input images. \n",
    "         Pooling layers reduce dimensionality and capture key features. \n",
    "         Fully connected layers combine extracted features for classification. \n",
    "         These components work together to recognize and classify handwritten digits or other images effectively.\n",
    "\n",
    "### 3. Discuss the advantages and limitations of LeNet-5 in the context of image classification tasks.\n",
    "-   Ans: LeNet-5's strengths lie in its pioneering use of CNNs for image classification. It excels at simple tasks like \n",
    "         handwritten digit recognition due to its convolution and pooling layers that capture basic features. \n",
    "         However, it struggles with complex and diverse images due to its limited depth and simplicity. \n",
    "         Modern architectures have surpassed LeNet-5 by incorporating deeper layers, advanced activation functions, \n",
    "         and larger datasets, enabling them to handle intricate image classification tasks more effectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3386ea-f74a-4f66-9375-b34f0a211148",
   "metadata": {},
   "source": [
    "### 4. Implement LeNet-5 using a deep learning framework of your choice (e.g., TensorFlow, PyTorch) and train it on a publicly available dataset (e.g., MNIST) Evaluate its performance and provide insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6d7fe06-2951-4d3e-b4de-8c9e2760d37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 03:59:27.636601: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-01 03:59:27.711629: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-01 03:59:27.713117: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-01 03:59:28.918911: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense,AveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96676881-a676-411f-990f-a63160ece8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "312aab04-0d5b-4a9b-8df5-92a1bb4fd032",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b8a6905-85b7-4bfb-8ffe-f3ada8d52448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 24, 24, 6)         156       \n",
      "                                                                 \n",
      " average_pooling2d (Average  (None, 12, 12, 6)         0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 16)          2416      \n",
      "                                                                 \n",
      " average_pooling2d_1 (Avera  (None, 4, 4, 16)          0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               30840     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 44426 (173.54 KB)\n",
      "Trainable params: 44426 (173.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the Model Architecture\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(6, kernel_size = (5,5), padding = 'valid', activation='tanh', input_shape = (28, 28, 1)))\n",
    "model.add(AveragePooling2D(pool_size= (2,2), strides = 2, padding = 'valid'))\n",
    "\n",
    "model.add(Conv2D(16, kernel_size = (5,5), padding = 'valid', activation='tanh'))\n",
    "model.add(AveragePooling2D(pool_size= (2,2), strides = 2, padding = 'valid'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(120, activation='tanh'))\n",
    "model.add(Dense(84, activation='tanh'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc292d82-3b02-427f-9c60-155d42d14893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "422/422 [==============================] - 6s 11ms/step - loss: 0.3724 - accuracy: 0.8936 - val_loss: 0.1491 - val_accuracy: 0.9605\n",
      "Epoch 2/10\n",
      "422/422 [==============================] - 4s 10ms/step - loss: 0.1380 - accuracy: 0.9583 - val_loss: 0.1033 - val_accuracy: 0.9672\n",
      "Epoch 3/10\n",
      "422/422 [==============================] - 4s 10ms/step - loss: 0.0935 - accuracy: 0.9721 - val_loss: 0.0730 - val_accuracy: 0.9782\n",
      "Epoch 4/10\n",
      "422/422 [==============================] - 4s 10ms/step - loss: 0.0688 - accuracy: 0.9791 - val_loss: 0.0623 - val_accuracy: 0.9822\n",
      "Epoch 5/10\n",
      "422/422 [==============================] - 4s 10ms/step - loss: 0.0556 - accuracy: 0.9832 - val_loss: 0.0597 - val_accuracy: 0.9820\n",
      "Epoch 6/10\n",
      "422/422 [==============================] - 4s 10ms/step - loss: 0.0438 - accuracy: 0.9864 - val_loss: 0.0496 - val_accuracy: 0.9855\n",
      "Epoch 7/10\n",
      "422/422 [==============================] - 4s 10ms/step - loss: 0.0360 - accuracy: 0.9886 - val_loss: 0.0475 - val_accuracy: 0.9865\n",
      "Epoch 8/10\n",
      "422/422 [==============================] - 4s 10ms/step - loss: 0.0304 - accuracy: 0.9910 - val_loss: 0.0478 - val_accuracy: 0.9873\n",
      "Epoch 9/10\n",
      "422/422 [==============================] - 4s 10ms/step - loss: 0.0263 - accuracy: 0.9919 - val_loss: 0.0468 - val_accuracy: 0.9870\n",
      "Epoch 10/10\n",
      "422/422 [==============================] - 4s 10ms/step - loss: 0.0226 - accuracy: 0.9929 - val_loss: 0.0513 - val_accuracy: 0.9853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fbd78b72830>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=10,  validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f09377ac-07c8-4a24-bd81-83865af2a7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0468 - accuracy: 0.9858\n",
      "Test accuracy: 0.98580002784729\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058295f-b4f2-4000-b618-313f33c46489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d511bb7-ef5d-4802-9e8e-d615fbc505a4",
   "metadata": {},
   "source": [
    "# TOPIC: Analyzing AlexNet\n",
    "\n",
    "### 1. Present an overview of the AlexNet architecture\n",
    "-    Ans: AlexNet is a significant Convolutional Neural Network (CNN) architecture known for revolutionizing image classification. \n",
    "         It features five convolutional and pooling layers for feature extraction and dimension reduction. Relu activation boosts non-linearity, \n",
    "         and dropout prevents overfitting. Fully connected layers combine features for classification. \n",
    "         AlexNet introduced GPU acceleration, ReLU activation, and data augmentation, leading to its victory in the ImageNet competition. \n",
    "         Its design laid the foundation for modern deep CNNs.\n",
    "\n",
    "### 2. Explain the architectural innovations introduced in AlexNet that contributed to its breakthrough performance.\n",
    "-    Ans: AlexNet introduced GPU acceleration for faster training, ReLU activation for better non-linearity, and data augmentation to expand the training set. It also implemented dropout to prevent overfitting. These innovations improved computation efficiency, enabled deeper networks, and enhanced the model's ability to capture complex features, leading to its remarkable success in image classification.\n",
    "\n",
    "### 3. Discuss the role of convolutional layers, pooling layers, and fully connected layers in AlexNet.\n",
    "-    Ans: In AlexNet, convolutional layers extract features like edges, textures, and patterns from images. \n",
    "          Pooling layers reduce dimensionality while keeping crucial information intact. \n",
    "          Fully connected layers merge these features for classification, determining what object the image contains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d973744-59db-4f2f-a21e-3e6c76e41651",
   "metadata": {},
   "source": [
    "### 4. Implement AlexNet using a deep learning framework of your choice and evaluate its performance on a dataset of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cc15ca6-f12f-4e9b-bc9d-1835a02e8ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-01 04:01:46.511618: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-01 04:01:47.100063: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-09-01 04:01:47.102901: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-01 04:01:49.021287: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "883c094f-31f8-482e-9d73-81491903e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "835e7a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c98f798-bd76-49d3-b664-c8d91f799e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 64)        640       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 13, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 13, 13, 64)        256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 13, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 6, 6, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 6, 6, 128)         512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 6, 6, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 6, 6, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 6, 6, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 6, 6, 256)         590080    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 3, 3, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 3, 3, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2304)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              9441280   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 4096)              16384     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 4096)              16384     \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                40970     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27849994 (106.24 MB)\n",
      "Trainable params: 27831690 (106.17 MB)\n",
      "Non-trainable params: 18304 (71.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create AlexNet model\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(4096, activation='relu'),\n",
    "    Dropout(0.4),\n",
    "    BatchNormalization(),\n",
    "\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "332860c2-e818-44e8-808c-79c092718503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebae9887-9b61-4c6e-830c-6f877dc512a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "422/422 [==============================] - 115s 266ms/step - loss: 0.1629 - accuracy: 0.9615 - val_loss: 0.8645 - val_accuracy: 0.7622\n",
      "Epoch 2/3\n",
      "422/422 [==============================] - 106s 251ms/step - loss: 0.0536 - accuracy: 0.9852 - val_loss: 0.0571 - val_accuracy: 0.9862\n",
      "Epoch 3/3\n",
      "422/422 [==============================] - 106s 251ms/step - loss: 0.0448 - accuracy: 0.9880 - val_loss: 0.0754 - val_accuracy: 0.9850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f7f368f8d30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4722879b-5786-465f-9ae4-2a95e9844ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 28ms/step - loss: 0.0621 - accuracy: 0.9861\n",
      "Test accuracy: 0.9861000180244446\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ae705-e055-4a72-b0ef-2499e8895c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f294f-c532-44ec-94eb-a40350b2cd21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c22505-a01d-4461-93c9-4fabd996ec1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
